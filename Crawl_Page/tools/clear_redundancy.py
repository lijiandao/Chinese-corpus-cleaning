import re
from html import unescape
from bs4 import BeautifulSoup
import subprocess

def remove_html_tags(html_content):
    """
    ä½¿ç”¨ BeautifulSoup ç§»é™¤ HTML æ ‡ç­¾
    :param html_content: åŒ…å« HTML æ ‡ç­¾çš„å­—ç¬¦ä¸²
    :return: ç§»é™¤ HTML æ ‡ç­¾åçš„æ–‡æœ¬
    """
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text()

def clean_text(text):
    """
    æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ç¬¦å·å’Œç©ºæ ¼ï¼Œå¹¶ç§»é™¤ç‰¹å®š Unicode èŒƒå›´çš„è¡¨æƒ…ç¬¦å·
    :param text: å¾…æ¸…ç†çš„æ–‡æœ¬
    :return: æ¸…ç†åçš„æ–‡æœ¬
    """
    # ç§»é™¤ emoji è¡¨æƒ…
    emoji_pattern = re.compile(
        "["
        "\U0001F300-\U0001F5FF"
        "\u2190-\u21FF"
        "\u2600-\u26FF"
        "\u2700-\u27BF"
        "\U0001F600-\U0001F6FF"
        "\U0001F700-\U0001F77F"
        "\U0001F900-\U0001F9FF"
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)

    # å…è®¸çš„æ ‡ç‚¹ç¬¦å·
    allowed_punctuation = " !~@#$%^&*()_+<>?:\"{}|,./;'[]\\-ï¼ï¿¥â€¦â€¦&*ï¼ˆï¼‰_+<>ï¼Ÿï¼š{}|ï¼Œã€‚ï¼Œï¼›ã€ã€‘â€”"

    # ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å…è®¸çš„æ ‡ç‚¹
    cleaned_chars = []
    for char in text:
        if re.match(r'[\u4e00-\u9fffA-Za-z0-9]', char):
            cleaned_chars.append(char)
        elif char in allowed_punctuation:
            cleaned_chars.append(char)
        # å¦åˆ™è¿‡æ»¤æ‰

    cleaned_text = ''.join(cleaned_chars)
    # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼ä¸ºä¸€ä¸ªç©ºæ ¼
    cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text)
    return cleaned_text.strip()

class SuffixAutomatonState:
    def __init__(self):
        self.length = 0
        self.link = -1
        self.next = dict()  # char -> stateId
        self.end_pos_set = set()  # æ”¶é›†å‡ºç°ä½ç½®

class SuffixAutomaton:
    def __init__(self):
        self.states = [SuffixAutomatonState()]
        self.last = 0

    def extend(self, c, pos):
        cur = len(self.states)
        self.states.append(SuffixAutomatonState())
        self.states[cur].length = self.states[self.last].length + 1
        self.states[cur].end_pos_set.add(pos)

        p = self.last
        while p != -1 and c not in self.states[p].next:
            self.states[p].next[c] = cur
            p = self.states[p].link
        if p == -1:
            self.states[cur].link = 0
        else:
            q = self.states[p].next[c]
            if self.states[p].length + 1 == self.states[q].length:
                self.states[cur].link = q
            else:
                clone = len(self.states)
                self.states.append(SuffixAutomatonState())
                self.states[clone].length = self.states[p].length + 1
                self.states[clone].next = self.states[q].next.copy()
                self.states[clone].link = self.states[q].link
                self.states[clone].end_pos_set = set(self.states[q].end_pos_set)
                while p != -1 and self.states[p].next[c] == q:
                    self.states[p].next[c] = clone
                    p = self.states[p].link
                self.states[q].link = clone
                self.states[cur].link = clone
        self.last = cur

    def consolidate_end_pos(self):
        # æŒ‰é•¿åº¦ä»å¤§åˆ°å°æ’åº
        order = list(range(len(self.states)))
        order.sort(key=lambda x: -self.states[x].length)
        for s in order:
            link = self.states[s].link
            if link != -1:
                self.states[link].end_pos_set.update(self.states[s].end_pos_set)

def remove_long_repeated_substrings(s):
    """
    ä»å­—ç¬¦ä¸² s ä¸­ï¼Œåˆ é™¤æ‰€æœ‰"é•¿åº¦â‰¥21"çš„é‡å¤å­ä¸²çš„ç¬¬2+æ¬¡å‡ºç°ï¼›
    ä¿ç•™æ¯ä¸ªé‡å¤å­ä¸²åœ¨æ–‡æœ¬ä¸­çš„ç¬¬ä¸€æ¬¡å‡ºç°ã€‚
    è¿”å›åˆ é™¤åå¾—åˆ°çš„å­—ç¬¦ä¸²ã€‚
    """
    # 1) æ„å»ºåç¼€è‡ªåŠ¨æœº
    automaton = SuffixAutomaton()
    for i, c in enumerate(s):
        automaton.extend(c, i)
    automaton.consolidate_end_pos()

    # 2) éå†æ‰€æœ‰çŠ¶æ€, è·å–æ»¡è¶³"lengthâ‰¥21 && å‡ºç°æ¬¡æ•°â‰¥2"çš„å­ä¸²çš„å‡ºç°åŒºé—´
    substring_occurrences_map = dict()
    states = automaton.states
    for st in states:
        if st.length < 21:
            continue
        if len(st.end_pos_set) < 2:
            continue
        substring_len = st.length
        link_len = 0 if st.link == -1 else states[st.link].length
        for end_pos in st.end_pos_set:
            start_pos = end_pos - substring_len + 1
            if start_pos < 0:
                continue
            sub = s[start_pos:end_pos + 1]
            if sub not in substring_occurrences_map:
                substring_occurrences_map[sub] = []
            substring_occurrences_map[sub].append((start_pos, end_pos))

    # 3) å¯¹æ¯ä¸ªé‡å¤å­ä¸²ï¼ˆsubï¼‰ï¼š
    #    - å‡ºç°åŒºé—´æŒ‰èµ·å§‹ä½ç½®æ’åº
    #    - ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°(ä¸åˆ é™¤)ï¼ŒæŠŠåç»­å‡ºç°çš„åŒºé—´éƒ½æ”¾è¿›"å¾…åˆ é™¤"çš„åˆ—è¡¨
    to_remove = []
    for sub, intervals in substring_occurrences_map.items():
        if len(intervals) < 2:
            continue
        intervals.sort(key=lambda x: x[0])
        for interval in intervals[1:]:
            to_remove.append(interval)

    if not to_remove:
        return s

    # 4) åˆå¹¶åŒºé—´
    to_remove.sort(key=lambda x: (x[0], x[1]))
    merged = []
    cur = list(to_remove[0])
    for nxt in to_remove[1:]:
        if nxt[0] <= cur[1] + 1:
            cur[1] = max(cur[1], nxt[1])
        else:
            merged.append(tuple(cur))
            cur = list(nxt)
    merged.append(tuple(cur))

    # 5) è·³è¿‡è¿™äº›åŒºé—´
    result_pieces = []
    index = 0
    for start, end in merged:
        if index < start:
            result_pieces.append(s[index:start])
        index = end + 1
    if index < len(s):
        result_pieces.append(s[index:])
    return ''.join(result_pieces)

def deduplicate_with_commoncrawl_dedupe(input_path, output_path, dedupe_path='./commoncrawl_dedupe'):
    """
    ç”¨ commoncrawl_dedupe å·¥å…·å¯¹æ–‡ä»¶å»é‡
    :param input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param dedupe_path: dedupe å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as fout:
        subprocess.run([dedupe_path], stdin=open(input_path, 'r', encoding='utf-8'), stdout=fout)


def remove_repeat(text):
    """
    ä½¿ç”¨ commoncrawl_dedupe å·¥å…·å¯¹æ–‡æœ¬å»é‡
    """
    with open('temp.txt', 'w', encoding='utf-8') as fout:
        fout.write(text)
    deduplicate_with_commoncrawl_dedupe('temp.txt', 'temp_deduped.txt')
    return open('temp_deduped.txt', 'r', encoding='utf-8').read()
# ----------------------
# ç®€å•æµ‹è¯•ä¸æ¼”ç¤º
# # ----------------------
# if __name__ == "__main__":
#     test_string = (
#         "abc...abc...è¿™é‡Œæ„é€ ä¸€äº›é‡å¤å­ä¸²...xyz...xyz"
#         "å†æ‹¼æ¥ä¸€å¤§æ®µæ–‡å­—ï¼Œç¡®ä¿é‡Œé¢æœ‰è¶…è¿‡20å­—ç¬¦çš„é‡å¤"
#         "å†æ‹¼æ¥ä¸€å¤§æ®µæ–‡å­—ï¼Œç¡®ä¿é‡Œé¢æœ‰è¶…è¿‡20å­—ç¬¦çš„é‡å¤"
#         "...ç»“å°¾å¤„ä¹Ÿå¯èƒ½æœ‰é‡å¤..."
#     )
#     result = remove_long_repeated_substrings(test_string)
#     print("åŸå­—ç¬¦ä¸²:", test_string)
#     print("åˆ é™¤é‡å¤å­ä¸²å:", result)

#     my_string = "This is some æµ‹è¯•æ–‡æœ¬ ğŸ¤”ğŸ™‚ğŸ˜ŠğŸ˜³,,,,,,è¡Œè€….slideshare.net"
#     print(remove_html_tags(clean_text(my_string)))
import re
from html import unescape
from bs4 import BeautifulSoup
import subprocess

def remove_html_tags(html_content):
    """
    ä½¿ç”¨ BeautifulSoup ç§»é™¤ HTML æ ‡ç­¾
    :param html_content: åŒ…å« HTML æ ‡ç­¾çš„å­—ç¬¦ä¸²
    :return: ç§»é™¤ HTML æ ‡ç­¾åçš„æ–‡æœ¬
    """
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text()

def clean_text(text):
    """
    æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ç¬¦å·å’Œç©ºæ ¼ï¼Œå¹¶ç§»é™¤ç‰¹å®š Unicode èŒƒå›´çš„è¡¨æƒ…ç¬¦å·
    :param text: å¾…æ¸…ç†çš„æ–‡æœ¬
    :return: æ¸…ç†åçš„æ–‡æœ¬
    """
    # ç§»é™¤ emoji è¡¨æƒ…
    emoji_pattern = re.compile(
        "["
        "\U0001F300-\U0001F5FF"
        "\u2190-\u21FF"
        "\u2600-\u26FF"
        "\u2700-\u27BF"
        "\U0001F600-\U0001F6FF"
        "\U0001F700-\U0001F77F"
        "\U0001F900-\U0001F9FF"
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)

    # å…è®¸çš„æ ‡ç‚¹ç¬¦å·
    allowed_punctuation = " !~@#$%^&*()_+<>?:\"{}|,./;'[]\\-ï¼ï¿¥â€¦â€¦&*ï¼ˆï¼‰_+<>ï¼Ÿï¼š{}|ï¼Œã€‚ï¼Œï¼›ã€ã€‘â€”"

    # ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å…è®¸çš„æ ‡ç‚¹
    cleaned_chars = []
    for char in text:
        if re.match(r'[\u4e00-\u9fffA-Za-z0-9]', char):
            cleaned_chars.append(char)
        elif char in allowed_punctuation:
            cleaned_chars.append(char)
        # å¦åˆ™è¿‡æ»¤æ‰

    cleaned_text = ''.join(cleaned_chars)
    # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼ä¸ºä¸€ä¸ªç©ºæ ¼
    cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text)
    return cleaned_text.strip()

class SuffixAutomatonState:
    def __init__(self):
        self.length = 0
        self.link = -1
        self.next = dict()  # char -> stateId
        self.end_pos_set = set()  # æ”¶é›†å‡ºç°ä½ç½®

class SuffixAutomaton:
    def __init__(self):
        self.states = [SuffixAutomatonState()]
        self.last = 0

    def extend(self, c, pos):
        cur = len(self.states)
        self.states.append(SuffixAutomatonState())
        self.states[cur].length = self.states[self.last].length + 1
        self.states[cur].end_pos_set.add(pos)

        p = self.last
        while p != -1 and c not in self.states[p].next:
            self.states[p].next[c] = cur
            p = self.states[p].link
        if p == -1:
            self.states[cur].link = 0
        else:
            q = self.states[p].next[c]
            if self.states[p].length + 1 == self.states[q].length:
                self.states[cur].link = q
            else:
                clone = len(self.states)
                self.states.append(SuffixAutomatonState())
                self.states[clone].length = self.states[p].length + 1
                self.states[clone].next = self.states[q].next.copy()
                self.states[clone].link = self.states[q].link
                self.states[clone].end_pos_set = set(self.states[q].end_pos_set)
                while p != -1 and self.states[p].next[c] == q:
                    self.states[p].next[c] = clone
                    p = self.states[p].link
                self.states[q].link = clone
                self.states[cur].link = clone
        self.last = cur

    def consolidate_end_pos(self):
        # æŒ‰é•¿åº¦ä»å¤§åˆ°å°æ’åº
        order = list(range(len(self.states)))
        order.sort(key=lambda x: -self.states[x].length)
        for s in order:
            link = self.states[s].link
            if link != -1:
                self.states[link].end_pos_set.update(self.states[s].end_pos_set)

def remove_long_repeated_substrings(s):
    """
    ä»å­—ç¬¦ä¸² s ä¸­ï¼Œåˆ é™¤æ‰€æœ‰"é•¿åº¦â‰¥21"çš„é‡å¤å­ä¸²çš„ç¬¬2+æ¬¡å‡ºç°ï¼›
    ä¿ç•™æ¯ä¸ªé‡å¤å­ä¸²åœ¨æ–‡æœ¬ä¸­çš„ç¬¬ä¸€æ¬¡å‡ºç°ã€‚
    è¿”å›åˆ é™¤åå¾—åˆ°çš„å­—ç¬¦ä¸²ã€‚
    """
    # 1) æ„å»ºåç¼€è‡ªåŠ¨æœº
    automaton = SuffixAutomaton()
    for i, c in enumerate(s):
        automaton.extend(c, i)
    automaton.consolidate_end_pos()

    # 2) éå†æ‰€æœ‰çŠ¶æ€, è·å–æ»¡è¶³"lengthâ‰¥21 && å‡ºç°æ¬¡æ•°â‰¥2"çš„å­ä¸²çš„å‡ºç°åŒºé—´
    substring_occurrences_map = dict()
    states = automaton.states
    for st in states:
        if st.length < 21:
            continue
        if len(st.end_pos_set) < 2:
            continue
        substring_len = st.length
        link_len = 0 if st.link == -1 else states[st.link].length
        for end_pos in st.end_pos_set:
            start_pos = end_pos - substring_len + 1
            if start_pos < 0:
                continue
            sub = s[start_pos:end_pos + 1]
            if sub not in substring_occurrences_map:
                substring_occurrences_map[sub] = []
            substring_occurrences_map[sub].append((start_pos, end_pos))

    # 3) å¯¹æ¯ä¸ªé‡å¤å­ä¸²ï¼ˆsubï¼‰ï¼š
    #    - å‡ºç°åŒºé—´æŒ‰èµ·å§‹ä½ç½®æ’åº
    #    - ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°(ä¸åˆ é™¤)ï¼ŒæŠŠåç»­å‡ºç°çš„åŒºé—´éƒ½æ”¾è¿›"å¾…åˆ é™¤"çš„åˆ—è¡¨
    to_remove = []
    for sub, intervals in substring_occurrences_map.items():
        if len(intervals) < 2:
            continue
        intervals.sort(key=lambda x: x[0])
        for interval in intervals[1:]:
            to_remove.append(interval)

    if not to_remove:
        return s

    # 4) åˆå¹¶åŒºé—´
    to_remove.sort(key=lambda x: (x[0], x[1]))
    merged = []
    cur = list(to_remove[0])
    for nxt in to_remove[1:]:
        if nxt[0] <= cur[1] + 1:
            cur[1] = max(cur[1], nxt[1])
        else:
            merged.append(tuple(cur))
            cur = list(nxt)
    merged.append(tuple(cur))

    # 5) è·³è¿‡è¿™äº›åŒºé—´
    result_pieces = []
    index = 0
    for start, end in merged:
        if index < start:
            result_pieces.append(s[index:start])
        index = end + 1
    if index < len(s):
        result_pieces.append(s[index:])
    return ''.join(result_pieces)

def deduplicate_with_commoncrawl_dedupe(input_path, output_path, dedupe_path='./commoncrawl_dedupe'):
    """
    ç”¨ commoncrawl_dedupe å·¥å…·å¯¹æ–‡ä»¶å»é‡
    :param input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param dedupe_path: dedupe å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as fout:
        subprocess.run([dedupe_path], stdin=open(input_path, 'r', encoding='utf-8'), stdout=fout)


def remove_repeat(text):
    """
    ä½¿ç”¨ commoncrawl_dedupe å·¥å…·å¯¹æ–‡æœ¬å»é‡
    """
    with open('temp.txt', 'w', encoding='utf-8') as fout:
        fout.write(text)
    deduplicate_with_commoncrawl_dedupe('temp.txt', 'temp_deduped.txt')
    return open('temp_deduped.txt', 'r', encoding='utf-8').read()
# ----------------------
# ç®€å•æµ‹è¯•ä¸æ¼”ç¤º
# # ----------------------
# if __name__ == "__main__":
#     test_string = (
#         "abc...abc...è¿™é‡Œæ„é€ ä¸€äº›é‡å¤å­ä¸²...xyz...xyz"
#         "å†æ‹¼æ¥ä¸€å¤§æ®µæ–‡å­—ï¼Œç¡®ä¿é‡Œé¢æœ‰è¶…è¿‡20å­—ç¬¦çš„é‡å¤"
#         "å†æ‹¼æ¥ä¸€å¤§æ®µæ–‡å­—ï¼Œç¡®ä¿é‡Œé¢æœ‰è¶…è¿‡20å­—ç¬¦çš„é‡å¤"
#         "...ç»“å°¾å¤„ä¹Ÿå¯èƒ½æœ‰é‡å¤..."
#     )
#     result = remove_long_repeated_substrings(test_string)
#     print("åŸå­—ç¬¦ä¸²:", test_string)
#     print("åˆ é™¤é‡å¤å­ä¸²å:", result)

#     my_string = "This is some æµ‹è¯•æ–‡æœ¬ ğŸ¤”ğŸ™‚ğŸ˜ŠğŸ˜³,,,,,,è¡Œè€….slideshare.net"
#     print(remove_html_tags(clean_text(my_string)))